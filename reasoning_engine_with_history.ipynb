{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lJAbLZ1ZEaHF",
      "metadata": {
        "id": "lJAbLZ1ZEaHF"
      },
      "source": [
        "# RAG with MongoDB Atlas and VertexAI Reasoning Engine using Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8095d1a",
      "metadata": {
        "id": "d8095d1a"
      },
      "source": [
        "**Vertex AI Reasoning Engine with Langchain**  is a powerful duo for building and deploying generative AI applications. It is one of the managed services in the Vertex AI console that porvides secure scalable runtime environment for your workload. Langchain provides the tool to design your application logic while reasoning engine provides environment to run it. With its flexibility to connect external data sources we can connect the MongoDB Atlas to Google VertexAI reasoning engine.\n",
        "\n",
        "At the core lies **MongoDB Atlas Vector Search**. It excels at searching unstructured data using vector embeddings, allowing you to find similar information even if phrased differently. This empowers your AI to grasp the true meaning behind user queries. Langchain then steps in, providing a user-friendly framework to design your application logic. Here, you can leverage Langchain's flexibility to seamlessly integrate MongoDB Atlas Vector Search, enabling your AI to retrieve highly relevant data based on semantic similarity. Finally, Vertex AI Reasoning Engine provides a secure and scalable environment to run your creations. This trio simplifies development, offering a pre-built foundation and tools to focus on building innovative solutions. With MongoDB Atlas Vector Search's semantic understanding, your generative AI applications can deliver superior results and user experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5REfyDmgEpzE",
      "metadata": {
        "id": "5REfyDmgEpzE"
      },
      "source": [
        "In this Notebook we will cover *How to build a RAG and deploy it as endpoints using Reasoning Engine, MongoDB Atlas and VertexAI*\n",
        "\n",
        "First we will install all thre required dependecies and restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ZKCZmNCsBK2W73bGDaZ61YMo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "executionInfo": {
          "elapsed": 20216,
          "status": "ok",
          "timestamp": 1720176050564,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "ZKCZmNCsBK2W73bGDaZ61YMo",
        "outputId": "eeda95b0-62ec-4373-c8dd-bab4585d48b8",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform[langchain,reasoningengine]\" \\\n",
        "    cloudpickle==3.0.0 \\\n",
        "    pydantic==2.7.4 \\\n",
        "    requests \\\n",
        "    datasets \\\n",
        "    pymongo \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-mongodb \\\n",
        "    langchain-google-vertexai \\\n",
        "    google-cloud-aiplatform \\\n",
        "    langchain_google_genai \\\n",
        "    requests \\\n",
        "    beautifulsoup4\n",
        "\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MYcbMsd2FDzr",
      "metadata": {
        "id": "MYcbMsd2FDzr"
      },
      "source": [
        "## Ingest data\n",
        "To begin with the setup we will load the dataset to MongoDB Atlas. For user convinience, we are using an existing Hugingface MongoDB embedding dataset. Run the below code to import the *MongoDB/subset_arxiv_papers_with_embeddings* dataset as ds and load to MongoDB Atlas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FNmiWoYLKNAi",
      "metadata": {
        "id": "FNmiWoYLKNAi"
      },
      "source": [
        "## Create vector search index on the newly created MongoDb collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RmuEkpybNLza",
      "metadata": {
        "id": "RmuEkpybNLza"
      },
      "source": [
        "// To do: add code for creating atlas vector search index on the collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.cloud import aiplatform\n",
        "from pymongo import MongoClient\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from pymongo import MongoClient\n",
        "import certifi\n",
        "from googleapiclient import discovery\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.agents.format_scratchpad.tools import format_to_tool_messages\n",
        "from langchain_core import prompts\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from vertexai.preview import reasoning_engines"
      ],
      "metadata": {
        "id": "_dEZOsmJo7CQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176176657,
          "user_tz": -330,
          "elapsed": 2055,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_dEZOsmJo7CQ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scrape the website content\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    content = ' '.join([p.text for p in soup.find_all('p')])\n",
        "    return content\n",
        "\n",
        "# Split the content into chunks of 1000 characters\n",
        "def split_into_chunks(text, chunk_size=1000):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "def get_text_embeddings(chunks):\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "    inputs = chunks[0]\n",
        "    embeddings = model.get_embeddings(chunks)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "def write_to_mongoDB(embeddings, chunks, db_name, coll_name):\n",
        "    client = MongoClient(\"mongodb+srv://venkatesh:ashwin123@freetier.kxcgwh2.mongodb.net/\", tlsCAFile=certifi.where())\n",
        "    db = client[db_name]\n",
        "    collection = db[coll_name]\n",
        "\n",
        "    for i in range(len(chunks)):\n",
        "        collection.insert_one({\n",
        "            \"chunk\": chunks[i],\n",
        "            \"embedding\": embeddings[i]\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "content = scrape_website(\"https://en.wikipedia.org/wiki/Star_Wars#The_Skywalker_Saga\")\n",
        "chunks = split_into_chunks(content)\n",
        "embeddings_starwars = get_text_embeddings(chunks)\n",
        "write_to_mongoDB(embeddings_starwars, chunks, \"embeddings\", \"sample_starwars_embeddings\")\n",
        "\n",
        "content = scrape_website(\"https://en.wikipedia.org/wiki/Star_Trek\")\n",
        "chunks = split_into_chunks(content)\n",
        "embeddings_starteck = get_text_embeddings(chunks)\n",
        "write_to_mongoDB(embeddings_starteck, chunks, \"embeddings\", \"sample_startrek_embeddings\")\n"
      ],
      "metadata": {
        "id": "k-mup_oYvuix"
      },
      "id": "k-mup_oYvuix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9df5222f",
      "metadata": {
        "id": "9df5222f"
      },
      "source": [
        "### Initilize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/apis/enableflow?apiid=aiplatform.googleapis.com)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0pifIeELue8b",
      "metadata": {
        "executionInfo": {
          "elapsed": 732,
          "status": "ok",
          "timestamp": 1720176123634,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "0pifIeELue8b"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"gcp-pov\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "STAGING_BUCKET = \"gs://vshanbh01\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b884ec",
      "metadata": {
        "id": "e8b884ec"
      },
      "source": [
        "### Import reasoning engine library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "BxpZ1MCBwkR8",
      "metadata": {
        "executionInfo": {
          "elapsed": 730,
          "status": "ok",
          "timestamp": 1720176128723,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "BxpZ1MCBwkR8"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview import reasoning_engines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28fe84de",
      "metadata": {
        "id": "28fe84de"
      },
      "source": [
        "### 1. Define Model\n",
        "\n",
        "As you construct your reasoning engine agent from the bottom up, the first component deals with which generative model you want to use in your agent. We are using \"gemini-1.5-pro\" which is latest at the time of creation of this python notebook. This LLM model will be used to build the RAG itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ouoPfB8OwsD1",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1720176130904,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "ouoPfB8OwsD1"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-1.5-pro-001\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "543c6e04",
      "metadata": {
        "id": "543c6e04"
      },
      "source": [
        "### 2. Defile Function to read from MongoDB Atlas using langchain\n",
        "\n",
        "The second component of your agent includes tools and functions, which enable the generative model to interact with MongoDB Atlas. We use Langchain to interact and query vectors from MongoDB Atlas. The function takes \"query\" as input and is trasformed into embeddings using Googles \"textembedding-gecko@001\" model. The function returns the queried data from MongoDB that has most similarity with the queried data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rq1nNT_fcMbV",
      "metadata": {
        "id": "rq1nNT_fcMbV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176134172,
          "user_tz": -330,
          "elapsed": 727,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "def star_wars_query_tool(\n",
        "    query: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves vectors from a MongoDB database and uses them to answer a question related to Star wars.\n",
        "\n",
        "    Args:\n",
        "        query: The question to be answered about star wars.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response to the question.\n",
        "    \"\"\"\n",
        "    from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "    from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "    from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "    from pymongo import MongoClient\n",
        "\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Do not return any answers from your own knowledge.\n",
        "\n",
        "\n",
        "    {context}\n",
        "\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Add your connection string in srv format below in place of URI\n",
        "    client = MongoClient(\"mongodb+srv://venkatesh:ashwin123@freetier.kxcgwh2.mongodb.net/\")\n",
        "    db = client[\"embeddings\"]\n",
        "\n",
        "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
        "\n",
        "    vs = MongoDBAtlasVectorSearch(\n",
        "        collection=db[\"sample_starwars_embeddings\"],\n",
        "        embedding=embeddings,\n",
        "        index_name=\"vector_index\",\n",
        "        embedding_key=\"embedding\",\n",
        "        text_key=\"chunk\",\n",
        "    )\n",
        "\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=1000,\n",
        "    )\n",
        "    retriever = vs.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.25}\n",
        "    )\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\", k=5, return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    response = conversation_chain({\"question\": query})\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def star_trek_query_tool(\n",
        "    query: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves vectors from a MongoDB database and uses them to answer a question related to star trek.\n",
        "\n",
        "    Args:\n",
        "        query: The question to be answered about star trek.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response to the question.\n",
        "    \"\"\"\n",
        "    from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "    from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "    from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "    from pymongo import MongoClient\n",
        "\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Do not return any answers from your own knowledge.\n",
        "\n",
        "\n",
        "    {context}\n",
        "\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Add your connection string in srv format below in place of URI\n",
        "    client = MongoClient(\"mongodb+srv://venkatesh:ashwin123@freetier.kxcgwh2.mongodb.net/\")\n",
        "    db = client[\"embeddings\"]\n",
        "\n",
        "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
        "\n",
        "    vs = MongoDBAtlasVectorSearch(\n",
        "        collection=db[\"sample_startrek_embeddings\"],\n",
        "        embedding=embeddings,\n",
        "        index_name=\"vector_index\",\n",
        "        embedding_key=\"embedding\",\n",
        "        text_key=\"chunk\",\n",
        "    )\n",
        "\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=1000,\n",
        "    )\n",
        "    retriever = vs.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.25}\n",
        "    )\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\", k=5, return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    response = conversation_chain({\"question\": query})\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "uc-ZiNxvEw1j",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176137059,
          "user_tz": -330,
          "elapsed": 718,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "uc-ZiNxvEw1j",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c9501830",
      "metadata": {
        "id": "c9501830"
      },
      "source": [
        "### 3. Define agent for calling your function\n",
        "\n",
        "The third component of your agent involves adding a reasoning layer, which helps your agent use the tools that you provided to help the end user achieve a higher-level goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt template\n",
        "prompt = {\n",
        "    \"history\": lambda x: x[\"history\"],\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": (lambda x: format_to_tool_messages(x[\"intermediate_steps\"])),\n",
        "} | prompts.ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        prompts.MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize session history\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "p_zpsbcUfN3A",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176184228,
          "user_tz": -330,
          "elapsed": 716,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "p_zpsbcUfN3A",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "B56uiBCPxDM0",
      "metadata": {
        "id": "B56uiBCPxDM0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176213358,
          "user_tz": -330,
          "elapsed": 682,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "agent = reasoning_engines.LangchainAgent(\n",
        "    prompt=prompt,\n",
        "    model=model,\n",
        "    chat_history=get_session_history,\n",
        "    model_kwargs={\"temperature\": 0},\n",
        "    tools=[star_wars_query_tool, star_trek_query_tool],\n",
        "    agent_executor_kwargs={\"return_intermediate_steps\": True},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your agent with a sample query."
      ],
      "metadata": {
        "id": "Oc5SoVfHjywT"
      },
      "id": "Oc5SoVfHjywT"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8frYrUr8I8rF",
      "metadata": {
        "id": "8frYrUr8I8rF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176361102,
          "user_tz": -330,
          "elapsed": 4491,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ef5fbceb-68d5-4a3d-b64b-28b84044565e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.base:Parent run 270d37d0-d3e2-4c4d-931e-cafcf4f04ff7 not found for run 751c1554-f7b1-4188-aa35-fa83fae97183. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Episode 1 of Star Wars is called **Star Wars: The Phantom Menace**. It was released in theaters on May 19, 1999. \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    input=\"Which was the episode 1 on star wars\",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")\n",
        "\n",
        "display(Markdown(response[\"output\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf896a3e",
      "metadata": {
        "id": "bf896a3e"
      },
      "source": [
        "### 4. Deploy the agent\n",
        "\n",
        "Now that you've specified a model, tools, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b8VyDfLbxQEU",
      "metadata": {
        "id": "b8VyDfLbxQEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176635021,
          "user_tz": -330,
          "elapsed": 229867,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0d84d8c8-da14-450e-ad58-76dc9cd9dc91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.reasoning_engines._reasoning_engines:Using bucket vshanbh01\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Writing to gs://vshanbh01/reasoning_engine/reasoning_engine.pkl\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Writing to gs://vshanbh01/reasoning_engine/requirements.txt\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Creating in-memory tarfile of extra_packages\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Writing to gs://vshanbh01/reasoning_engine/dependencies.tar.gz\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Creating ReasoningEngine\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Create ReasoningEngine backing LRO: projects/787220387490/locations/us-central1/reasoningEngines/3714238239557550080/operations/8440417262778712064\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:ReasoningEngine created. Resource name: projects/787220387490/locations/us-central1/reasoningEngines/3714238239557550080\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:To use this ReasoningEngine in another session:\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:reasoning_engine = vertexai.preview.reasoning_engines.ReasoningEngine('projects/787220387490/locations/us-central1/reasoningEngines/3714238239557550080')\n"
          ]
        }
      ],
      "source": [
        "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.7.4\",\n",
        "        \"langchain-mongodb\",\n",
        "        \"pymongo\",\n",
        "        \"langchain-google-vertexai\",\n",
        "\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grant Discovery Engine Editor access to Reasoning Engine service account\n",
        "\n",
        "Before you send queries to your remote agent, you'll need to grant the Discovery Engine Editor role to the Reasoning Engine service account.\n",
        "\n",
        "After you've completed this step, you remote agent will be able to retrieve documents from the data store that you created in Vertex AI Search:"
      ],
      "metadata": {
        "id": "zH0Sh8oIjV72"
      },
      "id": "zH0Sh8oIjV72"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the project number associated with your project ID\n",
        "service = discovery.build(\"cloudresourcemanager\", \"v1\")\n",
        "request = service.projects().get(projectId=PROJECT_ID)\n",
        "response = request.execute()\n",
        "project_number = response[\"projectNumber\"]\n",
        "project_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eXtpkG-ogUIL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720176666857,
          "user_tz": -330,
          "elapsed": 714,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "618daa70-6f48-4060-fbf5-61072b8a1a9f"
      },
      "id": "eXtpkG-ogUIL",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'787220387490'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hk09k1ehjlT0"
      },
      "id": "hk09k1ehjlT0"
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=serviceAccount:service-{project_number}@gcp-sa-aiplatform-re.iam.gserviceaccount.com \\\n",
        "    --role=roles/discoveryengine.editor"
      ],
      "metadata": {
        "id": "e-fFSPh3hPE1"
      },
      "id": "e-fFSPh3hPE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your remotely deployed agent\n",
        "With all of the core components of your community solar planning agent in place, you can send prompts to your remotely deployed agent to perform different tasks and test that it's working as expected"
      ],
      "metadata": {
        "id": "PmhKN_QJj5Zk"
      },
      "id": "PmhKN_QJj5Zk"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview import reasoning_engines\n",
        "\n",
        "\n",
        "REASONING_ENGINE_RESOURCE_NAME = \"projects/787220387490/locations/us-central1/reasoningEngines/3714238239557550080\"\n",
        "\n",
        "remote_agent = reasoning_engines.ReasoningEngine(REASONING_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"tell me about episode 1 of star wars\",\n",
        "    config={\"configurable\": {\"session_id\": \"mongodb demo\"}},\n",
        ")\n",
        "print(response[\"output\"])\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"When was it released?\",\n",
        "    config={\"configurable\": {\"session_id\": \"mongodb demo\"}},\n",
        ")\n",
        "print(response[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucplNCI4hSum",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720177779852,
          "user_tz": -330,
          "elapsed": 19431,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7e250acb-7a5a-4f63-baab-9fc3dba703f2"
      },
      "id": "ucplNCI4hSum",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode I: The Phantom Menace is the first film in the Star Wars prequel trilogy, released in 1999. It takes place 32 years before the events of Episode IV: A New Hope.\n",
            "\n",
            "**Plot Overview:**\n",
            "\n",
            "* The film follows the story of Qui-Gon Jinn, a Jedi Master, his apprentice Obi-Wan Kenobi, and a young slave boy named Anakin Skywalker. They are tasked with rescuing Queen Amidala of Naboo from the Trade Federation's blockade.\n",
            "* During their mission, they encounter Darth Maul, a Sith Lord who engages Obi-Wan and Qui-Gon in a lightsaber duel. Qui-Gon is killed by Darth Maul, but Obi-Wan defeats the Sith.\n",
            "* Anakin is revealed to be strong in the Force, and the Jedi believe he might be the Chosen One, a prophesied being who will bring balance to the Force.\n",
            "* Anakin is trained as a Jedi Padawan under Obi-Wan's guidance.\n",
            "* The film ends with the start of the Clone Wars, a conflict between the Republic and the Separatists.\n",
            "\n",
            "**Key Characters:**\n",
            "\n",
            "* **Qui-Gon Jinn:** A wise and experienced Jedi Master.\n",
            "* **Obi-Wan Kenobi:** Qui-Gon's Padawan, an aspiring Jedi Knight.\n",
            "* **Anakin Skywalker:** A young boy with an exceptionally strong connection to the Force.\n",
            "* **Darth Maul:** A Sith Lord, trained by Darth Sidious.\n",
            "* **Queen Amidala:** The ruler of Naboo.\n",
            "* **Palpatine:** A senator with a hidden identity as Darth Sidious, the Sith Lord who orchestrates the conflict.\n",
            "\n",
            "**Themes:**\n",
            "\n",
            "* The power of the Force\n",
            "* The importance of mentorship\n",
            "* The dangers of corruption and manipulation\n",
            "* The start of the Clone Wars, which leads to the rise of the Galactic Empire\n",
            "\n",
            "**Additional Facts:**\n",
            "\n",
            "* Episode I was the first Star Wars film to be released in theaters after more than 20 years.\n",
            "* It was a critical and commercial success, grossing over $1 billion worldwide.\n",
            "* The film introduced new characters and concepts that would become important in the Star Wars universe.\n",
            "\n",
            "Star Wars Episode I: The Phantom Menace was released on May 19, 1999. \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "reasoning_engine_langchain_class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}