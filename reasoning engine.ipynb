{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lJAbLZ1ZEaHF",
      "metadata": {
        "id": "lJAbLZ1ZEaHF"
      },
      "source": [
        "# RAG with MongoDB Atlas and VertexAI Reasoning Engine using Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5REfyDmgEpzE",
      "metadata": {
        "id": "5REfyDmgEpzE"
      },
      "source": [
        "In this Notebook we will cover *How to build a RAG and deploy it as endpoints using Reasoning Engine, MongoDB Atlas and VertexAI*\n",
        "\n",
        "First we will install all thre required dependecies and restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ZKCZmNCsBK2W73bGDaZ61YMo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "executionInfo": {
          "elapsed": 19528,
          "status": "ok",
          "timestamp": 1718715616217,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "ZKCZmNCsBK2W73bGDaZ61YMo",
        "outputId": "2580f891-89b4-4427-b0c2-3706a89d1cc1",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform[langchain,reasoningengine]\" \\\n",
        "    cloudpickle==3.0.0 \\\n",
        "    pydantic==2.7.4 \\\n",
        "    requests \\\n",
        "    datasets \\\n",
        "    pymongo \\\n",
        "    langchain \\\n",
        "    langchain-mongodb \\\n",
        "    langchain-google-vertexai \\\n",
        "\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MYcbMsd2FDzr",
      "metadata": {
        "id": "MYcbMsd2FDzr"
      },
      "source": [
        "## Ingest data\n",
        "To begin with the setup we will load the dataset to MongoDB Atlas. For user convinience, we are using an existing Hugingface MongoDB embedding dataset. Run the below code to import the *MongoDB/subset_arxiv_papers_with_embeddings* dataset as ds and load to MongoDB Atlas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OP-uRUPMFkXV",
      "metadata": {
        "collapsed": true,
        "id": "OP-uRUPMFkXV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "FNmiWoYLKNAi",
      "metadata": {
        "id": "FNmiWoYLKNAi"
      },
      "source": [
        "## Create vector search index on the newly created MongoDb collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RmuEkpybNLza",
      "metadata": {
        "id": "RmuEkpybNLza"
      },
      "source": [
        "// To do: add code for creating atlas vector search index on the collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0pifIeELue8b",
      "metadata": {
        "executionInfo": {
          "elapsed": 4361,
          "status": "ok",
          "timestamp": 1718715628153,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "0pifIeELue8b"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"gcp-pov\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "STAGING_BUCKET = \"gs://vshanbh01\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "BxpZ1MCBwkR8",
      "metadata": {
        "executionInfo": {
          "elapsed": 671,
          "status": "ok",
          "timestamp": 1718715632800,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "BxpZ1MCBwkR8"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview import reasoning_engines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ouoPfB8OwsD1",
      "metadata": {
        "executionInfo": {
          "elapsed": 644,
          "status": "ok",
          "timestamp": 1718715635977,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "ouoPfB8OwsD1"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-1.5-pro-001\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "rq1nNT_fcMbV",
      "metadata": {
        "executionInfo": {
          "elapsed": 657,
          "status": "ok",
          "timestamp": 1718716092270,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "rq1nNT_fcMbV"
      },
      "outputs": [],
      "source": [
        "def get_vectors_from_mongodb(\n",
        "    query: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves vectors from a MongoDB database and uses them to answer a question.\n",
        "\n",
        "    Args:\n",
        "        query: The question to be answered.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response to the question.\n",
        "    \"\"\"\n",
        "    from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "    from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "    from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "    from pymongo import MongoClient\n",
        "    import certifi\n",
        "\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Summarise the response in 2 sentences.\n",
        "\n",
        "\n",
        "    {context}\n",
        "\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Add your connection string in srv format below in place of URI\n",
        "    client = MongoClient(\"URI\", tlsCAFile=certifi.where())\n",
        "    db = client[\"vertexaiApp\"]\n",
        "\n",
        "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
        "\n",
        "    vs = MongoDBAtlasVectorSearch(\n",
        "        collection=db[\"chat-vec\"],\n",
        "        embedding=embeddings,\n",
        "        index_name=\"vector_index\",\n",
        "        embedding_key=\"vec\",\n",
        "        text_key=\"line\",\n",
        "    )\n",
        "\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=1000,\n",
        "    )\n",
        "    retriever = vs.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.25}\n",
        "    )\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\", k=5, return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    response = conversation_chain({\"question\": query})\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1FtMXk-zw-l9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4187,
          "status": "ok",
          "timestamp": 1718716098538,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "1FtMXk-zw-l9",
        "outputId": "e9a7a69d-a537-4c33-c65d-fba5884123ca"
      },
      "outputs": [],
      "source": [
        "get_vectors_from_mongodb(query=\"tell me about 04 Examples of collaborations with AWS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "B56uiBCPxDM0",
      "metadata": {
        "executionInfo": {
          "elapsed": 644,
          "status": "ok",
          "timestamp": 1718715514590,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "B56uiBCPxDM0"
      },
      "outputs": [],
      "source": [
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=model,\n",
        "    tools=[get_vectors_from_mongodb],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8frYrUr8I8rF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4844,
          "status": "ok",
          "timestamp": 1718716151526,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "8frYrUr8I8rF",
        "outputId": "4f77fb6a-ab5e-4002-f2d6-ea2b04f0a39e"
      },
      "outputs": [],
      "source": [
        "agent.query(input=\"tell me about 04 Examples of collaborations with AWS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8VyDfLbxQEU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8VyDfLbxQEU",
        "outputId": "78cf063f-96ab-433f-c428-6c94e9eb0f6b"
      },
      "outputs": [],
      "source": [
        "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.7.4\",\n",
        "        \"langchain-mongodb\",\n",
        "        \"pymongo\",\n",
        "        \"langchain-google-vertexai\",\n",
        "\n",
        "    ],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MYcbMsd2FDzr"
      ],
      "name": "reasoning engine",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
