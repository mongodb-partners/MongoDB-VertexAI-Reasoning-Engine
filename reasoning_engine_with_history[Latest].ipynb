{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lJAbLZ1ZEaHF",
      "metadata": {
        "id": "lJAbLZ1ZEaHF"
      },
      "source": [
        "# RAG with MongoDB Atlas and VertexAI Agent Engine using Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8095d1a",
      "metadata": {
        "id": "d8095d1a"
      },
      "source": [
        "**Vertex AI Agent Engine with Langchain**  is a powerful duo for building and deploying generative AI applications. It is one of the managed services in the Vertex AI console that porvides secure scalable runtime environment for your workload. Langchain provides the tool to design your application logic while reasoning engine provides environment to run it. With its flexibility to connect external data sources we can connect the MongoDB Atlas to Google VertexAI reasoning engine.\n",
        "\n",
        "At the core lies **MongoDB Atlas Vector Search**. It excels at searching unstructured data using vector embeddings, allowing you to find similar information even if phrased differently. This empowers your AI to grasp the true meaning behind user queries. Langchain then steps in, providing a user-friendly framework to design your application logic. Here, you can leverage Langchain's flexibility to seamlessly integrate MongoDB Atlas Vector Search, enabling your AI to retrieve highly relevant data based on semantic similarity. Finally, Vertex AI Agent Engine provides a secure and scalable environment to run your creations. This trio simplifies development, offering a pre-built foundation and tools to focus on building innovative solutions. With MongoDB Atlas Vector Search's semantic understanding, your generative AI applications can deliver superior results and user experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5REfyDmgEpzE",
      "metadata": {
        "id": "5REfyDmgEpzE"
      },
      "source": [
        "In this Notebook we will cover *How to build a RAG and deploy it as endpoints using Agent Engine, MongoDB Atlas and VertexAI*\n",
        "\n",
        "First we will install all thre required dependecies and restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ZKCZmNCsBK2W73bGDaZ61YMo",
      "metadata": {
        "id": "ZKCZmNCsBK2W73bGDaZ61YMo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform[langchain,agent_engines]\" requests datasets pymongo langchain langchain-community langchain-mongodb langchain-google-vertexai google-cloud-aiplatform langchain_google_genai requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPoL6TNlmodR",
        "outputId": "d5483ce8-3db2-4984-f7c5-144761d4f334"
      },
      "id": "qPoL6TNlmodR",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MYcbMsd2FDzr",
      "metadata": {
        "id": "MYcbMsd2FDzr"
      },
      "source": [
        "## Ingest data\n",
        "To begin with the setup we will load the dataset to MongoDB Atlas. For user convinience, we are using an existing Hugingface MongoDB embedding dataset. Run the below code to import the *MongoDB/subset_arxiv_papers_with_embeddings* dataset as ds and load to MongoDB Atlas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FNmiWoYLKNAi",
      "metadata": {
        "id": "FNmiWoYLKNAi"
      },
      "source": [
        "## Create vector search index on the newly created MongoDb collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RmuEkpybNLza",
      "metadata": {
        "id": "RmuEkpybNLza"
      },
      "source": [
        "// To do: add code for creating atlas vector search index on the collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.cloud import aiplatform\n",
        "from pymongo import MongoClient\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from pymongo import MongoClient\n",
        "import certifi\n",
        "from googleapiclient import discovery\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.agents.format_scratchpad.tools import format_to_tool_messages\n",
        "from langchain_core import prompts\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from vertexai.preview import reasoning_engines"
      ],
      "metadata": {
        "id": "_dEZOsmJo7CQ"
      },
      "id": "_dEZOsmJo7CQ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scrape the website content\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    content = ' '.join([p.text for p in soup.find_all('p')])\n",
        "    return content\n",
        "\n",
        "# Split the content into chunks of 1000 characters\n",
        "def split_into_chunks(text, chunk_size=1000):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "def get_text_embeddings(chunks):\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "    inputs = chunks[0]\n",
        "    embeddings = model.get_embeddings(chunks)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "def write_to_mongoDB(embeddings, chunks, db_name, coll_name):\n",
        "    client = MongoClient(\"MongoDB URI\", tlsCAFile=certifi.where())\n",
        "    db = client[db_name]\n",
        "    collection = db[coll_name]\n",
        "\n",
        "    for i in range(len(chunks)):\n",
        "        collection.insert_one({\n",
        "            \"chunk\": chunks[i],\n",
        "            \"embedding\": embeddings[i]\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "content = scrape_website(\"https://en.wikipedia.org/wiki/Star_Wars\")\n",
        "chunks = split_into_chunks(content)\n",
        "embeddings_starwars = get_text_embeddings(chunks)\n",
        "write_to_mongoDB(embeddings_starwars, chunks, \"REASONING-ENGINE\", \"sample_starwars_embeddings\")\n",
        "\n",
        "content = scrape_website(\"https://en.wikipedia.org/wiki/Star_Trek\")\n",
        "chunks = split_into_chunks(content)\n",
        "embeddings_starteck = get_text_embeddings(chunks)\n",
        "write_to_mongoDB(embeddings_starteck, chunks, \"REASONING-ENGINE\", \"sample_startrek_embeddings\")\n"
      ],
      "metadata": {
        "id": "k-mup_oYvuix"
      },
      "id": "k-mup_oYvuix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9df5222f",
      "metadata": {
        "id": "9df5222f"
      },
      "source": [
        "### Initilize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/apis/enableflow?apiid=aiplatform.googleapis.com)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0pifIeELue8b",
      "metadata": {
        "id": "0pifIeELue8b"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"gcp-pov\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "STAGING_BUCKET = \"gs://vshanbh01\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b884ec",
      "metadata": {
        "id": "e8b884ec"
      },
      "source": [
        "### Import Agent Engine library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "BxpZ1MCBwkR8",
      "metadata": {
        "id": "BxpZ1MCBwkR8"
      },
      "outputs": [],
      "source": [
        "from vertexai import agent_engines\n",
        "from vertexai.preview.reasoning_engines import LangchainAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28fe84de",
      "metadata": {
        "id": "28fe84de"
      },
      "source": [
        "### 1. Define Model\n",
        "\n",
        "As you construct your agent engine agent from the bottom up, the first component deals with which generative model you want to use in your agent. We are using \"gemini-1.5-pro\" which is latest at the time of creation of this python notebook. This LLM model will be used to build the RAG itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ouoPfB8OwsD1",
      "metadata": {
        "id": "ouoPfB8OwsD1"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-1.5-pro-001\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "543c6e04",
      "metadata": {
        "id": "543c6e04"
      },
      "source": [
        "### 2. Defile Function to read from MongoDB Atlas using langchain\n",
        "\n",
        "The second component of your agent includes tools and functions, which enable the generative model to interact with MongoDB Atlas. We use Langchain to interact and query vectors from MongoDB Atlas. The function takes \"query\" as input and is trasformed into embeddings using Googles \"textembedding-gecko@001\" model. The function returns the queried data from MongoDB that has most similarity with the queried data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rq1nNT_fcMbV",
      "metadata": {
        "id": "rq1nNT_fcMbV"
      },
      "outputs": [],
      "source": [
        "def star_wars_query_tool(\n",
        "    query: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves vectors from a MongoDB database and uses them to answer a question related to Star wars.\n",
        "\n",
        "    Args:\n",
        "        query: The question to be answered about star wars.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response to the question.\n",
        "    \"\"\"\n",
        "    from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "    from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "    from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "    from pymongo import MongoClient\n",
        "\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Do not return any answers from your own knowledge. Respond only in 2 or 3 sentences.\n",
        "\n",
        "\n",
        "    {context}\n",
        "\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Add your connection string in srv format below in place of URI\n",
        "    client = MongoClient(\"Update your URI here\")\n",
        "    db = client[\"REASONING-ENGINE\"]\n",
        "\n",
        "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
        "\n",
        "    vs = MongoDBAtlasVectorSearch(\n",
        "        collection=db[\"sample_starwars_embeddings\"],\n",
        "        embedding=embeddings,\n",
        "        index_name=\"vector_index\",\n",
        "        embedding_key=\"embedding\",\n",
        "        text_key=\"chunk\",\n",
        "    )\n",
        "\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=1000,\n",
        "    )\n",
        "    retriever = vs.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.25}\n",
        "    )\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\", k=5, return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    response = conversation_chain({\"question\": query})\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def star_trek_query_tool(\n",
        "    query: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves vectors from a MongoDB database and uses them to answer a question related to star trek.\n",
        "\n",
        "    Args:\n",
        "        query: The question to be answered about star trek.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response to the question.\n",
        "    \"\"\"\n",
        "    from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "    from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "    from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "    from pymongo import MongoClient\n",
        "\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Do not return any answers from your own knowledge. Respond only in 2 or 3 sentences.\n",
        "\n",
        "\n",
        "    {context}\n",
        "\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Add your connection string in srv format below in place of URI\n",
        "    client = MongoClient(\"Update Your URI here\")\n",
        "    db = client[\"REASONING-ENGINE\"]\n",
        "\n",
        "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
        "\n",
        "    vs = MongoDBAtlasVectorSearch(\n",
        "        collection=db[\"sample_startrek_embeddings\"],\n",
        "        embedding=embeddings,\n",
        "        index_name=\"vector_index\",\n",
        "        embedding_key=\"embedding\",\n",
        "        text_key=\"chunk\",\n",
        "    )\n",
        "\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=1000,\n",
        "    )\n",
        "    retriever = vs.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.25}\n",
        "    )\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\", k=5, return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    response = conversation_chain({\"question\": query})\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "uc-ZiNxvEw1j"
      },
      "id": "uc-ZiNxvEw1j",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c9501830",
      "metadata": {
        "id": "c9501830"
      },
      "source": [
        "### 3. Define agent for calling your function\n",
        "\n",
        "The third component of your agent involves adding a reasoning layer, which helps your agent use the tools that you provided to help the end user achieve a higher-level goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize session history\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "p_zpsbcUfN3A"
      },
      "id": "p_zpsbcUfN3A",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "B56uiBCPxDM0",
      "metadata": {
        "id": "B56uiBCPxDM0"
      },
      "outputs": [],
      "source": [
        "agent = LangchainAgent(\n",
        "    model=model,\n",
        "    chat_history=get_session_history,\n",
        "    model_kwargs={\"temperature\": 0},\n",
        "    tools=[star_wars_query_tool, star_trek_query_tool],\n",
        "    agent_executor_kwargs={\"return_intermediate_steps\": True},\n",
        ")\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your agent with a sample query."
      ],
      "metadata": {
        "id": "Oc5SoVfHjywT"
      },
      "id": "Oc5SoVfHjywT"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8frYrUr8I8rF",
      "metadata": {
        "id": "8frYrUr8I8rF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "adf3a612-71ff-4fab-fbe5-3caf02a8feb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-6f5ad85c885f>:56: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(\n",
            "<ipython-input-5-6f5ad85c885f>:65: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = conversation_chain({\"question\": query})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The main antagonist in the Star Wars series is Darth Vader, a dark lord of the Sith. He was originally played by David Prowse in the original trilogy, and later voiced by James Earl Jones. In the prequel trilogy, he appears as Anakin Skywalker, and was played by Hayden Christensen. \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    input=\"Who was the antagonist in Star wars and is played by whom? \",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")\n",
        "\n",
        "display(Markdown(response[\"output\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    input=\"Which episode does David Prowse becomes darth vader? \",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")"
      ],
      "metadata": {
        "id": "nXVUqoWbJJFg"
      },
      "id": "nXVUqoWbJJFg",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response[\"output\"]))"
      ],
      "metadata": {
        "id": "yeMTsjFvJUad"
      },
      "id": "yeMTsjFvJUad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf896a3e",
      "metadata": {
        "id": "bf896a3e"
      },
      "source": [
        "### 4. Deploy the agent\n",
        "\n",
        "Now that you've specified a model, tools, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b8VyDfLbxQEU",
      "metadata": {
        "id": "b8VyDfLbxQEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae55e55-f1e2-4c95-cf43-fb667597eef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.agent_engines:Identified the following requirements: {'google-cloud-aiplatform': '1.84.0', 'cloudpickle': '3.0.0'}\n",
            "INFO:vertexai.agent_engines:The final list of requirements: ['google-cloud-aiplatform[agent_engines,langchain]', 'cloudpickle==3.0.0', 'pydantic>=2.10', 'requests', 'langchain-mongodb', 'pymongo', 'langchain-google-vertexai']\n",
            "INFO:vertexai.agent_engines:Using bucket vshanbh01\n",
            "INFO:vertexai.agent_engines:Wrote to gs://vshanbh01/agent_engine/agent_engine.pkl\n",
            "INFO:vertexai.agent_engines:Writing to gs://vshanbh01/agent_engine/requirements.txt\n",
            "INFO:vertexai.agent_engines:Creating in-memory tarfile of extra_packages\n",
            "INFO:vertexai.agent_engines:Writing to gs://vshanbh01/agent_engine/dependencies.tar.gz\n",
            "INFO:vertexai.agent_engines:Creating AgentEngine\n",
            "INFO:vertexai.agent_engines:Create AgentEngine backing LRO: projects/787220387490/locations/us-central1/reasoningEngines/6930485672662794240/operations/8689450392997593088\n",
            "INFO:vertexai.agent_engines:View progress and logs at https://console.cloud.google.com/logs/query?project=gcp-pov\n",
            "INFO:vertexai.agent_engines:AgentEngine created. Resource name: projects/787220387490/locations/us-central1/reasoningEngines/6930485672662794240\n",
            "INFO:vertexai.agent_engines:To use this AgentEngine in another session:\n",
            "INFO:vertexai.agent_engines:agent_engine = vertexai.agent_engines.get('projects/787220387490/locations/us-central1/reasoningEngines/6930485672662794240')\n"
          ]
        }
      ],
      "source": [
        "remote_agent = agent_engines.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[agent_engines,langchain]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic>=2.10\",\n",
        "        \"requests\",\n",
        "        \"langchain-mongodb\",\n",
        "        \"pymongo\",\n",
        "        \"langchain-google-vertexai\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grant Discovery Engine Editor access to Agent Engine service account\n",
        "\n",
        "\n",
        "Before you send queries to your remote agent, you'll need to grant the Discovery Engine Editor role to the Reasoning Engine service account.\n",
        "\n",
        "After you've completed this step, you remote agent will be able to retrieve documents from the data store that you created in Vertex AI Search:"
      ],
      "metadata": {
        "id": "zH0Sh8oIjV72"
      },
      "id": "zH0Sh8oIjV72"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the project number associated with your project ID\n",
        "service = discovery.build(\"cloudresourcemanager\", \"v1\")\n",
        "request = service.projects().get(projectId=PROJECT_ID)\n",
        "response = request.execute()\n",
        "project_number = response[\"projectNumber\"]\n",
        "project_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eXtpkG-ogUIL",
        "outputId": "c11cc409-7ba2-452f-ef2f-c37f17e44389"
      },
      "id": "eXtpkG-ogUIL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'787220387490'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hk09k1ehjlT0"
      },
      "id": "hk09k1ehjlT0"
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=serviceAccount:service-{project_number}@gcp-sa-aiplatform-re.iam.gserviceaccount.com \\\n",
        "    --role=roles/discoveryengine.editor"
      ],
      "metadata": {
        "id": "e-fFSPh3hPE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "faf81000-c84d-4778-e426-1a92445ad0c4"
      },
      "id": "e-fFSPh3hPE1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [1] EXPRESSION=request.time < timestamp(\"2024-09-06T02:59:01.276Z\"), TITLE=cloudbuild-connection-setup\n",
            " [2] None\n",
            " [3] Specify a new condition\n",
            "The policy contains bindings with conditions, so specifying a condition is \n",
            "required when adding a binding. Please specify a condition.:  \n",
            "\n",
            "Command killed by keyboard interrupt\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your remotely deployed agent\n",
        "With all of the core components of your community solar planning agent in place, you can send prompts to your remotely deployed agent to perform different tasks and test that it's working as expected"
      ],
      "metadata": {
        "id": "PmhKN_QJj5Zk"
      },
      "id": "PmhKN_QJj5Zk"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview import reasoning_engines\n",
        "\n",
        "\n",
        "REASONING_ENGINE_RESOURCE_NAME = \"projects/787220387490/locations/us-central1/reasoningEngines/6930485672662794240\"\n",
        "\n",
        "remote_agent = reasoning_engines.ReasoningEngine(REASONING_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"tell me about episode 1 of star wars\",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")\n",
        "print(response[\"output\"])\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"Who was the main charecter in this series\",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")\n",
        "print(response[\"output\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucplNCI4hSum",
        "outputId": "2a253a80-23d7-420f-9b12-4177c6585fa7"
      },
      "id": "ucplNCI4hSum",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Star Wars: Episode I â€“ The Phantom Menace was the first film installment released as part of the prequel trilogy. It was released on May 19, 1999. The main plot lines involve the return of Darth Sidious, the Jedi's discovery of young Anakin Skywalker, and the invasion of Naboo by the Trade Federation. \n",
            "\n",
            "The main character in Star Wars is Luke Skywalker. He is a young farm boy who dreams of adventure and becomes a Jedi Knight. He fights against the evil Galactic Empire alongside his friends, Princess Leia and Han Solo. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = remote_agent.query(\n",
        "    input=\"what is the episode 1 of star trek?\",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")\n",
        "print(response[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64T1iuXYM9TK",
        "outputId": "1c94d9ec-aa4f-4ebf-a00e-105fc7ac2e86"
      },
      "id": "64T1iuXYM9TK",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 of Star Trek is called \"The Man Trap\". It was first aired on September 8, 1966. The story involves the Enterprise crew investigating the disappearance of a crew on a scientific outpost. It turns out that the crew members were killed by a creature that can take on someone else's form after it kills them. \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "reasoning_engine_with_history.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}